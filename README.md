![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python)
![Docker Compose](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker&logoColor=white)
![Apache Kafka](https://img.shields.io/badge/Apache-Kafka-231F20?logo=apachekafka)
![Apache Spark](https://img.shields.io/badge/Apache-Spark-E25A1C?logo=apachespark)
![Apache Airflow](https://img.shields.io/badge/Apache-Airflow-017CEE?logo=apacheairflow)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-336791?logo=postgresql)
![MinIO](https://img.shields.io/badge/MinIO-C72E49?logo=minio)
![S3 Compatible](https://img.shields.io/badge/S3-Compatible-orange?logo=amazonaws)
![Grafana](https://img.shields.io/badge/Grafana-F46800?logo=grafana)
![Prometheus](https://img.shields.io/badge/Prometheus-E6522C?logo=prometheus)


# üöÄ Hocotate Freight Data Platform

> _A full-stack space logistics simulation built for data engineering, streaming, and fun._

  

Hi! I‚Äôm the creator of this project, which is a silly simulation of the Pikmin games' **Hocotate Freight‚Äôs interplanetary shipping operations**, reimagined as a real, production-style **data platform**. ¬†

It‚Äôs a complete environment that models telemetry ingestion, mission registration, report generation, and real-time visualization...All using modern data engineering tooling!

  

This project runs on a hybrid setup:

  

- **Ships** (Python processes) operate locally, sending telemetry to HQ.

- **Headquarters** (Dockerized stack) handles Kafka streaming, MinIO object storage, Spark processing, Airflow ingestion, Postgres warehousing, and Grafana dashboards.

  

I built it to explore **end-to-end data flows**, **observability**, and **streaming architecture design**, is what I'd love to say on my resume. Honestly? I just wondered how Hocotate Freight's infra worked...So here we are!

  

---

  

## üõ∞Ô∏è Architecture Overview

  

```

¬† ¬† ¬†‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê

¬† ¬† ¬†‚îÇ ¬† ¬† ¬† ¬†LOCAL SHIP ¬† ¬† ¬† ¬†‚îÇ

¬† ¬† ¬†‚îÇ (deploy_ship.py, main.py)‚îÇ

¬† ¬† ¬†‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†‚îÇ ¬†Telemetry JSON

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†‚ñº

¬† ¬† ¬† ¬† ¬† ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê

¬† ¬† ¬† ¬† ¬† ‚îÇ ¬† Kafka ¬† ¬† ¬†‚îÇ

¬† ¬† ¬† ¬† ¬† ‚îÇ ¬†telemetry.raw

¬† ¬† ¬† ¬† ¬† ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†‚îÇ ¬†real-time stream

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†‚ñº

¬† ¬† ¬†‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê

¬† ¬† ¬†‚îÇ Spark Structured Streaming ‚îÇ

¬† ¬† ¬†‚îÇ (telemetry_stream.py) ¬† ¬† ¬†‚îÇ

¬† ¬† ¬†‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

¬† ¬† ¬† ¬† ¬† ¬† ‚îÇ ¬† ¬† ¬† ¬†‚îÇ

¬† ¬† ¬† ¬† ¬† ¬† ‚îÇ ¬† ¬† ¬† ¬†‚îÇ

¬† ¬† ¬†‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ¬† ¬†‚îÇ

¬† ¬† ¬†‚îÇ MinIO ¬† ¬†‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îò (raw telemetry lake)

¬† ¬† ¬†‚îÇ (s3://telemetry-lake)

¬† ¬† ¬†‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò

¬† ¬† ¬† ¬† ¬† ¬† ‚îÇ

¬† ¬† ¬†‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê

¬† ¬† ¬†‚îÇ Airflow (ll_ingest.py) ¬† ¬†‚îÇ

¬† ¬† ¬†‚îÇ ‚Üí Postgres (LL schema) ¬† ¬†‚îÇ

¬† ¬† ¬†‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

¬† ¬† ¬† ¬† ¬† ¬† ¬† ‚îÇ

¬† ¬† ¬†‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê

¬† ¬† ¬†‚îÇ Metabase / Grafana ¬† ¬†‚îÇ

¬† ¬† ¬†‚îÇ (fleet analytics) ¬† ¬† ‚îÇ

¬† ¬† ¬†‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

```

  

---

  

## üè¢ Headquarters Stack (Docker Compose)

  

|Service|Role|Ports|

|---|---|---|

|**Postgres**|Mission registry + data warehouse (`params`, `LL` schemas)|5432|

|**ShipsAPI (FastAPI)**|Fleet Command API for mission registration (`main.py`)|8000|

|**Kafka (Bitnami)**|Message broker for live telemetry|9092 / 9094|

|**MinIO**|Object storage for captain reports and telemetry parquet|9000 / 9001|

|**Spark**|Processes real-time telemetry ‚Üí MinIO|8082 / 4040 / 7077|

|**Airflow**|Ingests manifests from MinIO to Postgres|8081|

|**Metabase**|Business intelligence layer|3000|

|**Grafana + Prometheus**|Live fleet health metrics dashboard|9090 / 3001|

|**Adminer**|DB web access|8080|

  

Each container runs inside a shared Docker network (`hocotate-freight-data-platform_default`), meaning services reach each other via hostnames like `postgres`, `minio`, or `kafka`.

  

---

  

## üß≠ Local Ship Environment

Outside Docker, each ship is simulated by:

- `deploy_ship.py` ‚Äì registers mission with HQ and emits telemetry to Kafka.

- `ship_model.py` ‚Äì physics & subsystem drift logic.

- `health_rules.py` ‚Äì translates metrics into condition states (GOOD, MID, BAD, etc.).

- `ship_simulator.py` ‚Äì generates daily captain reports and uploads to MinIO.

- `telemetry_stream.py` (runs in HQ) ‚Äì continuously reads telemetry from Kafka.


I would have loved for ships to be deployed via batch, but it would make little to no business sense (since Pikmin's hocotate freight is mission-based).

You can launch one ship per console, so prepare a bunch!

Ships get launched using:

```bash
pip install -r requirements.txt

python deploy_ship.py --ship="{nameyourship}" --captain="{nameyourcaptain}" --target="{one_of_the_existing_planets}" --reportfreq=2

```

And about how they connect to Kafka...
  
```bash

KAFKA_BOOTSTRAP=localhost:9092

MISSIONS_API=http://localhost:8000/register_mission

```

That way, the ship is ‚Äúin the field,‚Äù but HQ is still reachable through exposed host ports. I did not feel like making an actual VPN layer to connect to a container and auth there with a company ID, so this will have to do. I'm sure Hocotate Freight wouldn't have the maturity to make that anyway...

Direct database connection is established via the shipsapi! Which is a simple python script with a few endpoints.
  

---

  

## üñ• Radar Visualization

  

**Frontend:**

- `index.html`, `main.js`, `style.css`

- Connects to `ws://localhost:8001/ws`

- Displays planets and live ship positions, colored by status:

¬† ¬† - üü® OK

¬† ¬† - üü• REPAIR

¬† ¬† - ‚ö´ FAILURE

  

**Backend:**

  

- `app.py` ‚Üí FastAPI server with Kafka consumer + WebSocket bridge.

- Listens to Kafka telemetry and streams JSON payloads to connected browsers.

  

Together they form a live **Fleet Operations Radar**, updating every second.

  

---

  

## ‚öôÔ∏è Database Schemas

  

- `params.missions` ‚Üí mission registry and lifecycle tracking.

- `LL.ship_state` ‚Üí coherent per-tick telemetry (health, position, etc.).

- `LL.manifests` ‚Üí uploaded daily reports from ships.

- `LL.mission_reports` ‚Üí itemized cargo/value logs per mission.

- `LL.treasure_reports` ‚Üí optional flavor layer for special finds.

  

SQL scripts:

  

```

init_params.sql ¬† ‚Üí Core mission registry

ll_init.sql ¬† ¬† ¬† ‚Üí Landing layer tables

```

  

---

  

## ü™£ Object Storage

  

Ships and Spark both write to MinIO under distinct buckets:

  

|Bucket|Purpose|

|---|---|

|`telemetry-lake`|Raw Spark parquet telemetry|

|`missions`|Uploaded daily captain reports|

|`logs`|Automated upload events and checksums|

  

The buckets could use schema enforcing and a querying layer to make it decent as a lake, but that will be the object of a following push. I Promise...!

  
  

---

  

## üìä Observability

  

Prometheus collects metrics via Pushgateway (from Spark telemetry batches):

  

- `ship_fuel_percent`

- `ship_coord_x/y/z`

- `ship_motor_temp_c`, `ship_engine_pressure_kpa`, etc.

  

Grafana provides a mission-control-style dashboard, with gauges, line graphs, and alerts for failed or low-health ships.

> ‚ÄúGrafana dashboards are automatically populated if you configure the Prometheus data source at http://prometheus:9090.
> This can be done inside the UI, too (which is what I've been doing for now).‚Äù

---

  

## üßæ Airflow DAG

  

`ll_ingest.py` runs every 15 minutes, scanning the `missions` bucket for new manifests:

  

- Loads `.json` manifest

- Reads corresponding Parquet file

- Inserts into Postgres (`LL.mission_reports`, `LL.ship_state`)

- Records ingest in `LL.manifests`

  

Business Layer (BL) and Serving Layer (SL) coming soon!

  

---

  

## üß± Running Headquarters

  

```bash

docker compose up -d

```

  

Then visit:

  

- Fleet Command API ‚Üí [http://localhost:8000](http://localhost:8000/)

- Airflow ‚Üí [http://localhost:8081](http://localhost:8081/) (admin / admin)

- Metabase ‚Üí [http://localhost:3000](http://localhost:3000/)

- Grafana ‚Üí [http://localhost:3001](http://localhost:3001/) (admin / admin)

- MinIO Console ‚Üí [http://localhost:9001](http://localhost:9001/)

- Adminer ‚Üí [http://localhost:8080](http://localhost:8080/)

  

**And run the code from `kfk/topic_creation.bash` inside the kafka container.**

  

---

  

## üöÄ Running a Ship (Local)

```bash
pip install -r requirements.txt

python deploy_ship.py --ship="{nameyourship}" --captain="{nameyourcaptain}" --target="{one_of_the_existing_planets}" --reportfreq=2

```
  

This registers a mission, begins emitting telemetry, and generates daily reports automatically. ¬†

Watch the ship live on the radar frontend at [http://localhost:8001](http://localhost:8001/).

  

Do make sure to check the planets list! Either way, here it is :

  

```Python
PLANET_COORDS = {
    # Starting point
    "Hocotate": np.array([0.0, 0.0, 0.0]),

    # Inner band
    "PNF-404": np.array([0.7e8, -1.3e8, -1.5e8]),
    "Karut":   np.array([1.2e8, 0.8e8, 0.1e8]),
    "Giya":    np.array([-1.5e8, 0.3e8, -0.6e8]),
    "Nijo":    np.array([0.7e8, -1.1e8, 0.2e8]),
    "Sozor":   np.array([-0.9e8, -0.6e8, 0.5e8]),

    # Mid band
    "Koppai":  np.array([6.8e8, 2.1e8, -1.2e8]),
    "Ohri":    np.array([5.2e8, -2.4e8, 2.9e8]),
    "Moyama":  np.array([-6.1e8, 3.5e8, 0.9e8]),
    "Flukuey": np.array([4.0e8, -3.8e8, -1.0e8]),
    "Enohay":  np.array([3.3e8, 1.9e8, 2.2e8]),
    "Mihama":  np.array([-3.7e8, -1.6e8, 2.6e8]),
    "Ooji":    np.array([7.5e8, 0.0e8, 0.8e8]),
    "Ogura":   np.array([-5.5e8, 2.2e8, -2.3e8]),

    # Outer band
    "Conohan": np.array([9.4e8, -4.1e8, 1.6e8]),
    "Ocobo":   np.array([-1.0e9, 3.8e8, -1.9e8]),
    "Tagwa":   np.array([1.12e9, 1.1e8, -3.4e8]),
    "Enohee":  np.array([-1.25e9, -0.2e9, 2.7e8]),
    "Neechki": np.array([1.35e9, -3.0e8, 0.0e8]),
    "Koodgio": np.array([-1.10e9, 4.5e8, 3.2e8]),
    "Maxima":  np.array([1.45e9, 0.0e8, 4.0e8])
}
```

  
Do note the ship starts its journey in Hocotate! And no, it does not go back (yet). I should track this and update the mission states to account for return at some point, but cut me some slack for now please!

  

---

  

## üåê Directory Structure

```

‚îú‚îÄ‚îÄ api/

‚îÇ ¬† ‚îú‚îÄ‚îÄ main.py ¬† ¬† ¬† ¬† ¬† ¬† # Fleet Command API (HQ)

‚îÇ ¬† ‚îú‚îÄ‚îÄ Dockerfile

‚îÇ ¬† ‚îú‚îÄ‚îÄ init_params.sql

‚îÇ ¬† ‚îî‚îÄ‚îÄ ll_init.sql

‚îÇ

‚îú‚îÄ‚îÄ radar/

‚îÇ ¬† ‚îú‚îÄ‚îÄ app.py ¬† ¬† ¬† ¬† ¬† ¬† ¬†# WebSocket + Kafka bridge

‚îÇ ¬† ‚îú‚îÄ‚îÄ index.html

‚îÇ ¬† ‚îú‚îÄ‚îÄ main.js

‚îÇ ¬† ‚îî‚îÄ‚îÄ style.css

‚îÇ

‚îú‚îÄ‚îÄ ships/

‚îÇ ¬† ‚îú‚îÄ‚îÄ deploy_ship.py

‚îÇ ¬† ‚îú‚îÄ‚îÄ ship_model.py

‚îÇ ¬† ‚îú‚îÄ‚îÄ health_rules.py

‚îÇ ¬† ‚îú‚îÄ‚îÄ ship_simulator.py

‚îÇ ¬† ‚îú‚îÄ‚îÄ transformer.py

‚îÇ ¬† ‚îú‚îÄ‚îÄ uploader.py

‚îÇ ¬† ‚îî‚îÄ‚îÄ missions.py

‚îÇ

‚îú‚îÄ‚îÄ airflow/dags/ll_ingest.py

‚îú‚îÄ‚îÄ spark/telemetry_stream.py

‚îú‚îÄ‚îÄ docker-compose.yml

‚îî‚îÄ‚îÄ README.md

```

  

---

  

**HOCOTATE FREIGHT CENTRAL OPERATIONS LOG ‚Äî CLASSIFIED**

  

> ‚ÄúSince Captain Olimar‚Äôs disappearance, we‚Äôve expanded our fleet monitoring system. ¬†

> Every ship now transmits real-time health metrics through subspace Kafka channels, ¬†

> synchronized by our Spark Observatory and archived in the Lunar MinIO Complex.‚Äù

  

**Core Infrastructure:**

- **Fleet Command Database** ‚Äî Mission Registry, managed by HQ Command API.

- **Spark Observatory** ‚Äî Analyzes telemetry; reports anomalies to Prometheus.

- **MinIO Storage Vault** ‚Äî Contains all captain reports and cargo manifests.

- **Airflow Orchestrator** ‚Äî Synchronizes data between field and HQ.

- **Grafana Console** ‚Äî Displays vessel health, trajectory, and signal drift.

- **Metabase Analytics Deck** ‚Äî For executive decision-making.

  

---

## üß∞ Personal TODO

- ¬†Add **return-to-base tracking** once ships complete missions (they don‚Äôt go back yet).

- ¬†**Business Layer (BL)** and **Serving Layer (SL)** batch jobs.

- ¬†**Enforce schemas**, add **Schema Registry setup** and add a **querying layer** in MinIO to make it a proper lake.

- **Make Grafana Templates** instead of assuming people will do their own...


> _"One day‚Ä¶ the fleet will fly back home."_ üöÄ